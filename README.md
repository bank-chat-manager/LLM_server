# LLM 서버

이 서버는 대규모 언어 모델(LLM)을 사용하여 상담 텍스트에 대한 요약 및 키워드 태깅을 포함한 AI 기반 분석을 제공합니다.

## 디렉토리 구조

```
LLM_server/
├── app/
│   ├── __init__.py
│   ├── main.py         # FastAPI 앱 초기화 및 라우터 설정
│   ├── schemas.py      # 데이터 유효성 검증을 위한 Pydantic 모델
│   ├── services.py     # 핵심 비즈니스 로직 (예: LLM 호출)
│   └── api/
│       ├── __init__.py
│       └── endpoints/
│           ├── __init__.py
│           ├── summary.py    # 상담 요약 API
│           └── tagging.py    # 키워드 태깅 API
├── core/
│   ├── __init__.py
│   └── config.py       # 설정 관리 (예: API 키)
├── tests/
│   ├── __init__.py
│   └── test_api.py     # API 테스트 코드
├── .env                # 환경 변수 파일 (예: API 키)
├── requirements.txt    # 프로젝트 의존성
└── README.md           # 이 파일
```

## 설치 및 실행

1.  **`LLM_server` 디렉토리로 이동:**

    ```bash
    cd LLM_server
    ```

2.  **가상 환경 생성 및 활성화:**

    ```bash
    # 가상 환경 생성
    python -m venv venv

    # Windows에서 활성화
    .\venv\Scripts\activate
    ```

3.  **필요한 라이브러리 설치:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **FastAPI 서버 실행:**

    ```bash
    uvicorn app.main:app --reload
    ```

5. **Ollama 설치 및 실행:**

    https://github.com/ollama/ollama

6. **qwen2:0.5b 다운로드**

    ```bash
    ollama pull 
    ```

서버가 성공적으로 실행되면, 터미널에 `http://127.0.0.1:8000` 주소에서 애플리케이션이 실행 중이라는 메시지가 나타납니다.

웹 브라우저에서 [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) 로 접속하면 자동으로 생성된 API 문서를 확인할 수 있으며, 여기에서 직접 API를 테스트해볼 수 있습니다.

## LLM 연동 방식 결정

로컬 환경에서 LLM을 구동하기 위한 도구들을 비교 분석하고, 우리 프로젝트에 가장 적합한 방식을 선택합니다.

### 로컬 LLM 구동 도구 비교: Ollama vs. LM Studio vs. Jan

| 항목 | Ollama | LM Studio | Jan |
| :--- | :--- | :--- | :--- |
| **핵심 특징** | **CLI(명령어) 중심**의 가볍고 빠른 개발자용 도구 | **GUI(그래픽) 중심**의 가장 대중적이고 사용자 친화적인 도구 | **GUI 중심**의 완전한 오픈소스 생태계를 지향하는 도구 |
| **사용 편의성** | **상(上):** 터미널에 익숙한 개발자에게는 가장 간단하고 빠름. | **최상(最上):** GUI를 통해 모델 검색, 다운로드, 채팅, 서버 실행까지 모든 것이 가능. **초심자에게 가장 좋음.** | **상(上):** LM Studio와 유사한 직관적인 GUI를 제공. |
| **API 서버 기능** | **기본 기능:** 설치 시 백그라운드에서 API 서버가 자동으로 실행됨. (OpenAI API와 호환) | **내장 기능:** GUI 내에서 'Start Server' 버튼을 눌러 API 서버를 켜는 방식. (OpenAI API와 호환) | **내장 기능:** LM Studio와 유사하게 GUI에서 서버를 켤 수 있음. (OpenAI API와 호환) |
| **핵심 장점** | - **가벼움, 스크립팅, 자동화**에 유리<br>- 서버 환경에 배포하기 좋음 | - **압도적인 모델 라이브러리 접근성**<br>- GUI에서 모델 성능/자원 사용량 확인 용이 | - **완전한 오픈소스**<br>- 로컬/클라우드 모델 동시 사용 가능 |
| **단점** | - GUI가 없어 모델과 채팅하려면 터미널 사용<br>- 모델 검색 기능이 없음 | - **GUI 자체가 오픈소스가 아님**<br>- CLI 기능이 상대적으로 약함 | - LM Studio에 비해 커뮤니티나 지원 모델이 아직은 적을 수 있음 |
| **추천 대상** | - **개발자, 서버 환경**<br>- 자동화된 워크플로우에 통합 희망 | - **입문자, 일반 사용자**<br>- 다양한 모델을 쉽게 체험하고 싶을 때 | - **오픈소스 원칙**이 중요한 사용자<br>- LM Studio의 대안을 찾을 때 |

---

### 최종 선택: Ollama

분석 결과, 이 프로젝트에서는 **Ollama**를 사용하기로 결정했습니다.

1.  **프로젝트 목적 부합:** 우리는 사용자가 직접 모델과 채팅하는 프로그램이 아닌, FastAPI 서버가 안정적으로 호출할 **백엔드 AI API**를 구축하고 있습니다. GUI 없이 가볍고 빠르게 API를 제공하는 Ollama가 이 목적에 가장 적합합니다.
2.  **개발 및 배포 용이성:** CLI 기반이므로 개발 환경 구성과 추후 서버 배포 자동화에 유리합니다.
3.  **아키텍처의 일관성:** `LLM_server`는 비즈니스 로직에 집중하고, 무거운 모델 추론은 Ollama가 전담하는 구조는 전체 시스템이 지향하는 마이크로서비스 아키텍처(MSA)와 완벽하게 일치합니다.

## 모델 선정 과정

초기 개발 단계에서 사용할 경량 LLM으로 `gemma:2b`를 선택했으며, 그 과정은 다음과 같습니다.

### `gemma:2b` 모델 분석

- **요약(Summarization) 성능: 좋음**
  - `gemma:2b`는 간결하고 핵심적인 요약을 생성하는 데 강점이 있습니다.
  - 일부 벤치마크에서는 다른 경량 모델보다 높은 성능을 보이기도 했습니다.
  - 프로젝트의 '상담 내용 요약' 기능에 충분한 성능을 보일 것으로 기대됩니다.

- **키워드 추출(Keyword Extraction) 성능: 불확실**
  - 이 작업에 대한 직접적인 성능 측정 자료는 부족합니다.
  - Gemma 계열 모델이 정해진 출력 형식을 정확히 따르지 못하는 경향이 있다는 일부 평가가 있어, 프롬프트 엔지니어링을 통한 보완이 필요할 수 있습니다.

### 다른 모델과의 비교

- **Phi-3-mini:** `gemma:2b`와 비슷한 체급에서 매우 높은 성능으로 주목받는 모델입니다.
- **Llama-3-8B:** `gemma:2b`보다 크고 강력하지만, 더 높은 하드웨어 사양을 요구합니다. 추론 능력과 지시 사항 준수 능력이 더 뛰어납니다.

### 최종 전략

**결론: `gemma:2b`로 시작하는 것이 현재 단계에서 가장 효율적인 전략입니다.**

1.  **신속한 개발:** `gemma:2b`는 매우 가볍고 빨라, 로컬 환경에서 전체 API 흐름을 신속하게 개발하고 테스트하는 데 최적입니다.
2.  **실용적 검증:** 먼저 `gemma:2b`를 연동하여 실제 데이터로 결과물의 품질을 확인합니다.
3.  **점진적 개선:** 만약 `gemma:2b`의 성능이 부족할 경우, 더 강력한 `phi-3-mini`나 `llama3:8b` 같은 모델로 쉽게 교체할 수 있습니다. Ollama를 사용하므로 모델 교체 비용이 매우 낮습니다.

이러한 이유로, **"`gemma:2b`로 빠르게 프로토타입을 완성하고, 필요시 더 성능 좋은 모델로 업그레이드한다"** 는 점진적인 개발 방식을 채택합니다.